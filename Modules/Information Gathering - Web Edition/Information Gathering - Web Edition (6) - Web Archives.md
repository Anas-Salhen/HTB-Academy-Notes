# Web Archives
## The Wayback Machine
It's a digital archive that has been archiving websites since 1996. This allows users to go back in time and view snapshots of websites at different points in time.

## How It Works
1. `Crawling`: The Wayback Machine employs automated web crawlers (bots) to browse the internet. However, instead of just reading the content, these bots download copies of the webpages they encounter.
2. `Archiving`: The downloaded webpages, along with their associated resources like images, stylesheets, and scripts, are stored in the Wayback Machine's vast archive. Each captured webpage is linked to a specific date and time. This archiving process happens at regular intervals, sometimes daily, weekly, or monthly, depending on the website's popularity and frequency of updates.
3. `Accessing`: Users can access these archived snapshots through the Wayback Machine's interface. By entering a website's URL and selecting a date.

Website owners can request that their content be excluded from the Wayback Machine, although this is not always guaranteed.

## Why It Matters For Recon
1. `Uncovering Hidden Assets and Vulnerabilities`: It allows you to discover old web pages, directories, files, or subdomains that might not be accessible on the current website, potentially exposing sensitive information or security flaws.
2. `Tracking Changes and Identifying Patterns`: By comparing historical snapshots, you can observe how the website has evolved, revealing changes in structure, content, technologies, and potential vulnerabilities.
3. `Gathering Intelligence`: Archived content can be a valuable source of OSINT, providing insights into the target's past activities, marketing strategies, employees, and technology choices.
4. `Stealthy Reconnaissance`: Accessing archived snapshots is a passive activity that doesn't directly interact with the target's infrastructure, making it a less detectable way to gather information.
